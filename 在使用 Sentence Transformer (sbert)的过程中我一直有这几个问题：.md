åœ¨ä½¿ç”¨ Sentence Transformer (sbert)çš„è¿‡ç¨‹ä¸­æˆ‘ä¸€ç›´æœ‰è¿™å‡ ä¸ªé—®é¢˜ï¼š

1. sbert åˆ°åº•åšäº†ä»€ä¹ˆï¼Œä½¿å¾—å®ƒåœ¨å¥å­åµŒå…¥ï¼ˆsentence embeddingsï¼‰æ–¹é¢é¥é¥é¢†å…ˆï¼Ÿ
2. é™¤äº† sbertï¼Œå¥å­åµŒå…¥æ–¹æ³•è¿˜æœ‰åˆ«çš„é€‰æ‹©å—ï¼Ÿå°¤å…¶æ˜¯ä¸­æ–‡å¥å­çš„åµŒå…¥è¿˜æœ‰åˆ«çš„é€‰æ‹©å—ï¼Ÿ
3. sbert å’Œå®é™…ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯ä»€ä¹ˆå…³ç³»ï¼Œä¸ºä»€ä¹ˆæœ‰é‚£å¤šé¢„è®­ç»ƒæ¨¡å‹éƒ½é€šè¿‡ sbertåŠ è½½ã€‚

æˆ‘å°è¯•åœ¨ SO æé—® â€œ[What does SentenceTransformers provide to simplify sentence embedding?](https://stackoverflow.com/questions/79022351/what-does-sentencetransformers-provide-to-simplify-sentence-embedding)â€ ä¹Ÿæ²¡æœ‰äººå›å¤ã€‚éšä¾¿è¯´ä¸€å¥è¯ï¼ŒMLç›¸å…³é—®é¢˜SOä¼¼ä¹è¿˜ä¸æ˜¯åˆé€‚Q&Aè®ºå›ï¼Œä½†æˆ‘åˆæ²¡æœ‰æ‰¾åˆ°å“ªé‡Œå¯ä»¥æ›´åˆé€‚å‘é—®ã€‚

æˆ‘è‡ªå·±æµ‹è¯•å‘ç°æ™ºæºçš„â€œBAAI/bge-base-zh-v1.5" æ¨¡å‹å¯¹ä¸­æ–‡å¥å­çš„æ–‡æœ¬åµŒå…¥è¡¨ç°æœ€å¥½ï¼Œè¶…è¿‡äº† [sbert æåˆ°çš„å„ç§å¤šè¯­è¨€æ¨¡å‹](https://sbert.net/docs/sentence_transformer/pretrained_models.html#multilingual-models)ã€‚å¯æ˜¯å®ƒä¸æ˜¯ç”¨sentence bert æ¨¡å‹è®­ç»ƒçš„ï¼Œä¸ºä»€ä¹ˆå¯ä»¥ç”¨ Sentence Transformer æ¥åŠ è½½ï¼Ÿ

åœ¨æ¯”è¾ƒè¯•ç”¨å¦ä¸€ä¸ªä¸­æ–‡RAGæ¡†æ¶ [MaxKB](https://github.com/1Panel-dev/MaxKB) æˆ‘å‘ç°å®ƒç”¨äº†å¦ä¸€ä¸ªå¥å­åµŒå…¥åº“[text2vec](https://github.com/shibing624/text2vec). text2vec ä»‹ç»äº†å››ç§æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼ˆä»–è¯´çš„**æ–‡æœ¬å‘é‡**å°±æ˜¯å¥å­å‘é‡ï¼‰ï¼ŒåŒ…æ‹¬sbertï¼Œtext2vec ç”¨å…¶ä¸­çš„ [Cosine Sentence](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py) è®­ç»ƒï¼Œè‡ªç§°æ¯” sentence bertæ¨¡å‹æ•ˆæœå¥½ï¼Œä½†æ˜¯å®ƒä¹Ÿèƒ½é€šè¿‡ sbertåŠ è½½ã€‚

å¦‚æœè¿›ä¸€æ­¥ç‚¹å¼€sbert é¢„è®­ç»ƒæ¨¡å‹åœ¨ hugging faceçš„è¯´æ˜ï¼Œä»¥æœ€å¸¸è§çš„ [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) ä¸ºä¾‹ï¼Œå®ƒä»¬å±…ç„¶éƒ½æåˆ°æ¨¡å‹æ—¢å¯ä»¥é€šè¿‡ Sentence-Transformers åŠ è½½ï¼Œä¹Ÿå¯ä»¥ä¸é€šè¿‡ Sentence-Transformers ç›´æ¥ç”¨ HuggingFace Transformers åŠ è½½ï¼š

Without [sentence-transformers](https://www.sbert.net/), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

è€Œä¸”ä¸ç”¨ sbertåŠ è½½æ—¶å€™ç»™çš„ç¤ºä¾‹ä»£ç éƒ½ä¸€æ ·ï¼Œéƒ½éœ€è¦é€šè¿‡ `mean_pooling å‡½æ•°å¾—åˆ°å¥å­åµŒå…¥ã€‚è¿™ä¸ªmean_poolingåˆ°åº•åšäº†ä»€ä¹ˆï¼Ÿ`

HuggingFace è¿˜æœ‰ä¸€ç¯‡ä»‹ç»embeddingçš„å…¥é—¨æ–‡ç« [ Getting Started With Embeddings](https://huggingface.co/blog/getting-started-with-embeddings) ç®€å•è§£é‡Šäº†åµŒå…¥ï¼Œå¥å­ç›¸ä¼¼åº¦ï¼ˆè¿™äº›æˆ‘éƒ½æ˜ç™½ï¼‰ï¼Œä½†æ²¡èƒ½å¸®æˆ‘ç†è§£æˆ‘çš„ä¸‰ä¸ªç–‘é—®ã€‚ä¸è¿‡åœ¨å¯»æ‰¾ç­”æ¡ˆçš„è¿‡ç¨‹ä¸­æˆ‘çªç„¶æ„è¯†åˆ°ä¸€ç‚¹ï¼šæˆ‘ä»¬æœ‰å¾ˆå¤šç§æ–¹æ³•å¾—åˆ°**è¯åµŒå…¥**ï¼ŒåŒ…æ‹¬åœ¨transformerå‡ºç°å‰çš„ Word2vec å’Œ doc2vec, ä½†æ˜¯å¦‚ä½•ä»è¯åµŒå…¥(word embedding) å¾—åˆ°å¥å­åµŒå…¥(sentence embedding)ï¼Ÿæ€»ä¸èƒ½æŠŠä¸€ä¸ªå¥å­ä¸­çš„æ¯ä¸€ä¸ªè¯çš„è¯åµŒå…¥å‘é‡å¤šå åŠ å§ï¼

ä»”ç»†ç¿»çœ‹ text2vec é¡¹ç›®çš„æ–‡ç« ï¼Œæ³¨æ„åˆ° [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•) è¿™ç¯‡çŸ­æ–‡ï¼Œå‘ç°ä»¥ä¸‹çš„è¯å°è¯æˆ‘çš„åˆæ­¥ç†è§£ï¼ˆå¦‚ä½•ä»è¯å‘é‡å¾—åˆ°å¥å­å‘é‡ï¼‰ï¼Œâ€œ*å½“é€šè¿‡å¹³å‡è¯å‘é‡çš„æ–¹å¼è®¡ç®—å¥å‘é‡â€*:

- BERTè‡ªèº«å¯¼å‡ºçš„å¥å‘é‡ï¼ˆä¸ç»è¿‡Fine-tuneï¼Œ**å¯¹æ‰€æœ‰è¯å‘é‡æ±‚å¹³å‡**ï¼‰è´¨é‡è¾ƒä½ï¼Œç”šè‡³æ¯”ä¸ä¸ŠGloveçš„ç»“æœï¼Œå› è€Œéš¾ä»¥åæ˜ å‡ºä¸¤ä¸ªå¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦

> *ä¸»è¦åŸå› æ˜¯ï¼š*
>
> *1.BERTå¯¹æ‰€æœ‰çš„å¥å­éƒ½å€¾å‘äºç¼–ç åˆ°ä¸€ä¸ªè¾ƒå°çš„ç©ºé—´åŒºåŸŸå†…ï¼Œè¿™ä½¿å¾—å¤§å¤šæ•°çš„å¥å­å¯¹éƒ½å…·æœ‰è¾ƒé«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œå³ä½¿æ˜¯é‚£äº›è¯­ä¹‰ä¸Šå®Œå…¨æ— å…³çš„å¥å­å¯¹ã€‚*
>
> *2.BERTå¥å‘é‡è¡¨ç¤ºçš„èšé›†ç°è±¡å’Œå¥å­ä¸­çš„é«˜é¢‘è¯æœ‰å…³ã€‚å…·ä½“æ¥è¯´ï¼Œå½“é€šè¿‡å¹³å‡è¯å‘é‡çš„æ–¹å¼è®¡ç®—å¥å‘é‡æ—¶ï¼Œé‚£äº›é«˜é¢‘è¯çš„è¯å‘é‡å°†ä¼šä¸»å¯¼å¥å‘é‡ï¼Œä½¿ä¹‹éš¾ä»¥ä½“ç°å…¶åŸæœ¬çš„è¯­ä¹‰ã€‚å½“è®¡ç®—å¥å‘é‡æ—¶å»é™¤è‹¥å¹²é«˜é¢‘è¯æ—¶ï¼Œèšé›†ç°è±¡å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¾—åˆ°ç¼“è§£ï¼Œä½†è¡¨å¾èƒ½åŠ›ä¼šä¸‹é™ã€‚*

æ‰€ä»¥æˆ‘æŠ±ç€è¯•è¯•çœ‹çš„æƒ³æ³•ç»™ [sbertå¼€äº†ä¸€ä¸ªé—®é¢˜å•](https://github.com/UKPLab/sentence-transformers/issues/2958)ï¼Œè¯¢é—® sbertåˆ°åº•åšäº†ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆè¿™ä¹ˆå¤šæ¨¡å‹éƒ½å¯ä»¥é€šè¿‡sbertåŠ è½½ï¼›è€Œä¸ç”¨sbertåŠ è½½ç›´æ¥ç”¨ hugging face transfomerè®¡ç®—å¥å­çš„åµŒå…¥å‘é‡ï¼Œä¸ºä»€ä¹ˆå®ƒä»¬çš„ä»£ç éƒ½æ˜¯ç±»ä¼¼çš„ã€‚æ²¡æƒ³åˆ°å¾ˆå¿«å°±å¾—åˆ°ç­”å¤ï¼Œç­”å¤å°è¯äº†æˆ‘çš„æ€è€ƒï¼Œwe use a pooling strategy to go **from token embeddings -> text embeddings**. Sentence Transformers just abstracts away this conversion.

ä¸ºä»€ä¹ˆä¸ç”¨Sentence Transformersè®­ç»ƒçš„æ¨¡å‹ä¹Ÿèƒ½é€šè¿‡ Sentence TransformersåŠ è½½ï¼š

models not trained with Sentence Transformers, like the BGE models, can still be adapted to work in Sentence Transformers as long as the model 1) can be loaded with AutoModel and 2) produces token embeddings.

â€¦

In short: Sentence Transformers is a wrapper around Transformers that 1) abstracts away common embedding model functionality such as pooling and normalization for easy 3-line usage (import; load; encode) and 2) implements a whole range of finetuning functionality that can help you get the best model on exactly your data.

ç­”å¤è¿˜å»ºè®®æˆ‘è¦è¯»è¯»[sbertçš„è®ºæ–‡](https://arxiv.org/pdf/1908.10084)ï¼Œæ‰èƒ½æ›´æ¸…æ¥šçš„äº†è§£å®ƒçš„åŸç†ï¼Œè¿™æ˜¯æˆ‘ä¸€ç›´é€ƒé¿çš„äº‹ğŸ˜‚ï¼Œè§‰å¾—è¿™æŠŠå¹´çºªè¯»paperå®åœ¨åƒåŠ›äº†ï¼Œä¸è¿‡æ—¢ç„¶ä»–è¿™ä¹ˆè¯´ï¼Œå¥½æ­¹æŠŠæ‘˜è¦çœ‹äº† ï¼ˆçœ‹å®Œå°±å‘ç°ï¼Œä¸å°‘ä¸­æ–‡æ–‡ç« åœ¨è§£é‡Šsbertå°±ä»æ‘˜è¦é‡Œæ‘˜å‡ºè¿™ä¸¤å¥ï¼‰ï¼šâ€œ ä½¿ç”¨BERTåœ¨10,000ä¸ªå¥å­çš„é›†åˆä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸€å¯¹ï¼Œéœ€è¦å¤§çº¦5000ä¸‡æ¬¡æ¨ç†è®¡ç®—ï¼ˆ~65å°æ—¶ï¼‰ã€‚BERTçš„æ„é€ ä½¿å…¶ä¸é€‚åˆè¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢ä»¥åŠèšç±»ç­‰æ— ç›‘ç£ä»»åŠ¡ã€‚æˆ‘ä»¬ä»‹ç»äº† Sentence-BERT ï¼ˆSBERTï¼‰ï¼Œè¿™æ˜¯é¢„è®­ç»ƒ BERT ç½‘ç»œçš„ä¿®æ”¹ç‰ˆï¼Œå°†æŸ¥æ‰¾æœ€ç›¸ä¼¼å¯¹çš„å·¥ä½œé‡ä» BERT / RoBERTa çš„ 65 å°æ—¶å‡å°‘åˆ°ä½¿ç”¨ SBERT çš„çº¦ 5 ç§’ï¼ŒåŒæ—¶ä¿æŒ BERT çš„å‡†ç¡®æ€§ã€‚â€

æ–‡ç« è¿˜è§£é‡Šè®¡ç®—å¥å­è¿å…¥ç»å¸¸ä¼šç¢°åˆ°åˆ°å‡ ä¸ªæœ¯è¯­ï¼Œä¸å¦‚ mean pooling, CLS-token (æˆ‘æ²¡çœ‹æ‡‚ï¼Œè´´åœ¨è¿™é‡Œè­¦ç¤ºè‡ªå·±è¦åŠªåŠ›)ï¼Œä½†ç»ˆäºçŸ¥é“ **æ± åŒ–ç­–ç•¥** åŸæ¥å°±æ˜¯ **pooling strategy**

When trained with the regression objective function, we observe that the **pooling strategy** has a large impact. There, the MAX strategy perform significantly worse than MEAN or CLS-token strategy. This is in contrast to Conneau et al. ([2017](https://ar5iv.labs.arxiv.org/html/1908.10084?_immersive_translate_auto_translate=1#bib.bib10)), who found it beneficial for the BiLSTM-layer of InferSent to use MAX instead of MEAN pooling.

çŸ¥é“è¿™äº›å…³é”®è¯æˆ‘åˆè¿›ä¸€æ­¥æœåˆ°è¿™ç¯‡æ–‡ç«  [Sentence Embeddings. Introduction to Sentence Embeddings](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/) æ˜¯æˆ‘ç›®å‰æ‰¾åˆ°çš„å…³äºå¥å­è¿å…¥æœ€å…¨è§£é‡Šæ–‡ç« ï¼Œä½†å¦‚æœæ²¡æœ‰ä¹‹å‰çš„èƒŒæ™¯çŸ¥è¯†ï¼Œè¯»èµ·æ¥ä¹Ÿä¼šä¸å°‘å›°éš¾ã€‚æ–‡ç« æœ‰ä¸€ä¸ªç« èŠ‚ä¸“é—¨è§£é‡Šäº†è¿™äº›ç­–ç•¥ä»è¯å‘é‡å¾—åˆ°å¥å­å‘é‡: `[CLS]` pooling, max pooling and mean pooling.

CLS tokenéœ€è¦ä¸“é—¨æä¸€ä¸‹ï¼Œåœ¨BERTæ¨¡å‹ä¸­ï¼Œæ¯ä¸ªè¾“å…¥æ–‡æœ¬åœ¨è¢«é€å…¥æ¨¡å‹å‰ä¼šå¢åŠ ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼ˆtokenï¼‰ï¼Œå³CLSï¼ˆclassificationï¼‰æ ‡è®°ã€‚è¿™ä¸ªCLSæ ‡è®°æ”¾åœ¨è¾“å…¥åºåˆ—çš„å¼€å¤´ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†åœ¨æ¨¡å‹å¤„ç†å®Œè¾“å…¥æ–‡æœ¬åï¼Œèƒ½å¤Ÿä»è¿™ä¸ªç‰¹å®šçš„æ ‡è®°ä¸Šè·å¾—æ•´ä¸ªè¾“å…¥æ–‡æœ¬çš„è¡¨ç¤ºã€‚ [What is purpose of the [CLS\] token and why is its encoding output important?](https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important) æˆ‘å½“ç„¶ä¹Ÿæ˜¯ä¸€çŸ¥åŠè§£

åœ¨æˆ‘çš„è¿™æ®µå­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä¹Ÿæ‰¾åˆ°è‹±æ–‡å¥å­åµŒå…¥å¦å¤–ä¸¤ä¸ªé€‰æ‹©ï¼Œå‚è€ƒ [Top 4 Sentence Embedding Techniques using Python](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/)

1. facebook çš„ InferSent ï¼Œ åŸºäº [(Stanford Natural Language Inference) dataset](https://nlp.stanford.edu/projects/snli/) ä½†æ˜¯è¿™ä¸ªé¡¹ç›®è¿˜æ˜¯å…³åœï¼Œåº”è¯¥ç”¨çš„äººä¸å¤šï¼Œè€Œä¸”è¿™ä¸ªæ•°æ®é›†åº”è¯¥ä¸é€‚åˆä¸­æ–‡
2. googleçš„ Universal Sentence Encoderï¼Œ ç”¨çš„äººå¯èƒ½ä¹Ÿä¸å¤šï¼Œæ˜¯ sbert çš„é—®é¢˜å• [whatâ€™s the difference between USE and then SBERT?](https://github.com/UKPLab/sentence-transformers/issues/64) ç®€å•ç­”å¤ SBERT works much better

è¿™å°±æ˜¯æˆ‘ç›®å‰å­¦ä¹  sentence embedding çš„æ€»ç»“å¿ƒå¾—ã€‚

